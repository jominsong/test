pip install faiss-cpu PyPDF2 openai tiktoken
# pages/3_ChatPDF.py
import streamlit as st
import faiss
import numpy as np
import openai
from PyPDF2 import PdfReader
import tiktoken

# ì„ë² ë”© ê´€ë ¨ ì„¤ì •
EMBED_MODEL = "text-embedding-ada-002"
DIMENSIONS = 1536  # ada-002 output dimension

# ì„¸ì…˜ ì´ˆê¸°í™”
if "index" not in st.session_state:
    st.session_state.index = None
if "chunks" not in st.session_state:
    st.session_state.chunks = []

# PDF ì—…ë¡œë“œ
st.title("ğŸ“„ ChatPDF - ê·œì • ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸")
api_key = st.text_input("OpenAI API Key", type="password")
uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

# Clear ë²„íŠ¼
if st.button("ğŸ§¹ Clear"):
    st.session_state.index = None
    st.session_state.chunks = []
    st.success("ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ")

# PDF ì²˜ë¦¬ ë° ë²¡í„°í™”
if uploaded_file and api_key:
    openai.api_key = api_key
    pdf = PdfReader(uploaded_file)
    full_text = ""
    for page in pdf.pages:
        text = page.extract_text()
        if text:
            full_text += text.strip().replace("\n", " ") + "\n"

    # í…ìŠ¤íŠ¸ ë¶„í• 
    tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
    tokens = tokenizer.encode(full_text)
    chunk_size = 500
    token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]
    text_chunks = [tokenizer.decode(tc) for tc in token_chunks]

    # ì„ë² ë”©
    embeddings = []
    for chunk in text_chunks:
        resp = openai.Embedding.create(input=[chunk], model=EMBED_MODEL)
        vector = np.array(resp["data"][0]["embedding"], dtype="float32")
        embeddings.append(vector)

    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    index = faiss.IndexFlatL2(DIMENSIONS)
    index.add(np.array(embeddings))

    st.session_state.index = index
    st.session_state.chunks = text_chunks
    st.success("âœ… PDF ì„ë² ë”© ì™„ë£Œ! ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

# ì‚¬ìš©ì ì§ˆë¬¸ â†’ ê²€ìƒ‰ + GPT ë‹µë³€
if st.session_state.index and api_key:
    question = st.chat_input("PDF ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”!")
    if question:
        # ì§ˆë¬¸ ì„ë² ë”©
        query_embed = openai.Embedding.create(input=[question], model=EMBED_MODEL)["data"][0]["embedding"]
        query_vec = np.array(query_embed, dtype="float32")

        # ìœ ì‚¬ë„ ê²€ìƒ‰
        D, I = st.session_state.index.search(np.array([query_vec]), k=3)
        related_chunks = [st.session_state.chunks[i] for i in I[0]]

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context = "\n\n".join(related_chunks)
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì€ ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ì¼ë¶€ì…ë‹ˆë‹¤:\n\n{context}\n\nì‚¬ìš©ì ì§ˆë¬¸: {question}\n\nì´ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •ë¦¬ëœ ë‹µë³€ì„ í•´ì¤˜."

        # GPT ì‘ë‹µ ìƒì„±
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì—…ë¡œë“œëœ PDF ë¬¸ì„œë¥¼ ì˜ ì´í•´í•˜ëŠ” ë„ìš°ë¯¸ì•¼."},
                {"role": "user", "content": prompt}
            ]
        )

        answer = response["choices"][0]["message"]["content"]
        st.chat_message("user").markdown(question)
        st.chat_message("assistant").markdown(answer)
